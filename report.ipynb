{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project report - Continous Control\n",
    "\n",
    "In this project the goal is to train a robotic arm to reach a  goal in space. Most work on reinforced learning agents are on discrete action spaces to e.g. learn how to play Atari games from pixels (like DQN - Deep Q-learning). The problem with robotic control is that the action space is continous - which is a more challenging task than discrete actions due to a possible infinite action space. \n",
    "\n",
    "\n",
    "## The DDPG algorithm\n",
    "\n",
    "A algorithm called DDPG is developed to learn problems that has continous action spaces (controlling the motors and forces of the robot) and can be seen as DQN with continious actions. The robot model returns a state vector with all the info about the state of the robotic arm (not pixels) of 33 real values. The action vector (4 real values) is applied the model which replies with reward value and a new state vector. This is what we need to create a reinforced training agent that can learn the behaviour of the robotic arm without a model (model-free).\n",
    "\n",
    "DDPG is an actor-critic method. The actor estimates the best actions to take and the critic estimates the Q(s, a) value for the state/action pairs. The Q(s, a) is learn using the Bellman equation as in Q-learning. \n",
    "\n",
    "The actor is updated\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agent parameters\n",
    "\n",
    "BUFFER_SIZE = int(1e6)  # replay buffer size\n",
    "BATCH_SIZE = 256        # minibatch size\n",
    "GAMMA = 0.99            # discount factor\n",
    "TAU = 1e-3              # for soft update of target parameters\n",
    "LR_ACTOR = 1e-3         # learning rate of the actor \n",
    "LR_CRITIC = 1e-3        # learning rate of the critic\n",
    "WEIGHT_DECAY = 0        # L2 weight decay\n",
    "EPSILON = 1.0           # epsilon for the noise process added to the actions\n",
    "EPSILON_DECAY = 1e-6    # decay for epsilon above\n",
    "NOISE_SIGMA = 0.05      # sigma for Ornstein-Uhlenbeck noise\n",
    "UPDATE_EVERY = 20       # timesteps between updates\n",
    "NUM_UPDATES = 10        # num of update passes when updating\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nework parameters\n",
    "The neural networks used in this solution two hidden layers of 256 nodes each. I tried with other sizes but this seems sufficent for training.\n",
    "\n",
    "I added batch normalization between the layers with good effect on the speed of training (reaching goal in 133 vs. 199 epsisodes without batch normalization).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Future improvements\n",
    "\n",
    "There are ways to improve this even further by parallelizing the training using:\n",
    " * A2C\n",
    " * A3C\n",
    " \n",
    " * PPO\n",
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drlnd",
   "language": "python",
   "name": "drlnd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
